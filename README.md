# LLM Recall Testing (Needle in a Haystack)

English | [ç®€ä½“ä¸­æ–‡](README_CN.md)

## Test Results
Below are the relevant test results:
### Gemini Full Context Window View
![Full Context Window Test Results](test_results/Gall.png)

## Project Overview

This project tests the recall capabilities of mainstream large language models using a "needle in a haystack" methodology. This testing approach may correlate with the performance of model attention mechanisms to some extent.

### Tested Models

Primary models tested:
- **gemini-2.5-pro**
- **gemini-2.5-flash-preview-09-2025**
- **claude-sonnet-4-5-thinking**
- **DeepSeek_V3_0324**
- **gemini-deepthink**

Additionally, we tested recently popular hidden models:
- **lithiumflow**
- **orionmist**

âš ï¸ **Note**: Test data for all models except the hidden ones has been open-sourced and stored in the [`test_results/`](test_results/) directory.

## Testing Methodology

### Core Principle

The testing process follows these steps:

1. **Construct Test Text**: Randomly insert multiple four-digit numbers (1000-9999) into a fixed-token-length context
2. **Model Task**: Require the model to extract all four-digit numbers and output them in JSON format in order of appearance
3. **Scoring Algorithm**: Use an algorithm based on Edit Distance (Levenshtein Distance) to score model responses

### Scoring Algorithm

The scoring system is based on the **Edit Distance** algorithm and comprehensively evaluates model performance:

- âœ… **Penalizes extra keys** (Hallucination errors): Model outputs non-existent numbers
- âœ… **Penalizes missing keys** (Omission errors): Model misses numbers that should be extracted
- âœ… **Penalizes incorrect values**: Extracted number values are incorrect
- âœ… **Penalizes order errors**: Number sequence doesn't match the original text

**Accuracy Calculation Formula**:
```
Accuracy = (1 - Edit Distance / Max Sequence Length) Ã— 100%
```

## Project Structure

```
.
â”œâ”€â”€ æ”¶é›†æ•°æ®/                      # Data generation and collection module
â”‚   â”œâ”€â”€ generate_text.py           # Generate test text
â”‚   â”œâ”€â”€ run_batch_test.py          # Batch API testing script
â”‚   â”œâ”€â”€ numbers.json               # Standard answers
â”‚   â”œâ”€â”€ output.md                  # Generated test text
â”‚   â””â”€â”€ æ•°æ®åº“/                    # Test results database
â”‚
â”œâ”€â”€ æ•°æ®åˆ†æ/                      # Data analysis module
â”‚   â”œâ”€â”€ analyze_database.py        # Basic database analysis
â”‚   â”œâ”€â”€ analyze_summary.py         # Model overview statistics
â”‚   â”œâ”€â”€ analyze_errors.py          # Error type analysis (misorder/hallucination/missing)
â”‚   â”œâ”€â”€ analyze_position_accuracy.py # Position accuracy analysis (LCS algorithm)
â”‚   â”œâ”€â”€ create_heatmap.py          # Generate position accuracy heatmap
â”‚   â”œâ”€â”€ create_hallucination_heatmap.py # Generate hallucination error heatmap
â”‚   â”œâ”€â”€ create_missing_heatmap.py  # Generate missing error heatmap
â”‚   â”œâ”€â”€ create_misorder_position_heatmap.py # Generate misorder heatmap
â”‚   â”œâ”€â”€ generate_all_heatmaps.py   # Batch generate all heatmaps
â”‚   â”œâ”€â”€ grading_utils.py           # Scoring utility functions
â”‚   â””â”€â”€ åˆ†æç»“æœ/                  # Analysis results database
â”‚
â”œâ”€â”€ test_results/                  # Test results (classified by model)
â”‚   â”œâ”€â”€ gemini-2.5-pro/
â”‚   â”œâ”€â”€ gemini_2_5_flash_preview_09_2025/
â”‚   â”œâ”€â”€ claude_sonnet_4_5_thinking/
â”‚   â”œâ”€â”€ lithiumflow/
â”‚   â””â”€â”€ orionmist/
â”‚
â”œâ”€â”€ grading_utils.py               # Core scoring algorithm
â”œâ”€â”€ evaluate_test.py               # Single test evaluation
â”œâ”€â”€ ç­”æ¡ˆ.json                      # Standard answer example
â”œâ”€â”€ test.json                      # Test answer example
â””â”€â”€ README.md                      # Project documentation
```

## Usage Guide

### 1. Generate Test Data

Use [`generate_text.py`](æ”¶é›†æ•°æ®/generate_text.py) to generate test text:

```bash
cd æ”¶é›†æ•°æ®
python generate_text.py [context_length] [insertion_count]
```

**Parameters**:
- `context_length`: Base string length (default: 50000)
- `insertion_count`: Number of four-digit numbers to insert (default: 40)

**Example**:
```bash
python generate_text.py 30000 50  # Generate 30000 characters, insert 50 numbers
```

### 2. Batch Testing

Use [`run_batch_test.py`](æ”¶é›†æ•°æ®/run_batch_test.py) for batch API testing:

```bash
cd æ”¶é›†æ•°æ®
python run_batch_test.py [runs] [concurrency] [delay] [context_length] [insertions] [base_pattern]
```

**Parameters**:
- `runs`: Number of test runs (default: 10)
- `concurrency`: Number of concurrent requests (default: 10)
- `delay`: Delay between requests in seconds (default: 0)
- `context_length`: Context byte count (default: 30000)
- `insertions`: Number of numbers to insert (default: 40)
- `base_pattern`: Base string pattern (default: `"a|"`)

**Example**:
```bash
python run_batch_test.py 20 5 1 30000 50  # 20 tests, 5 concurrent, 1 second delay
```

**Note**: Configuration required in the script:
- API URL (`API_URL`)
- Model ID (`MODEL_ID`)
- API Key (`HEADERS['authorization']`)

### 3. Data Analysis

#### Basic Statistical Analysis

```bash
python æ•°æ®åˆ†æ/analyze_database.py <database_path>
```

#### Generate Model Overview

```bash
python æ•°æ®åˆ†æ/analyze_summary.py <database_path>
```

#### Error Type Analysis

Analyze three types of errors (misorder, hallucination, missing):

```bash
python æ•°æ®åˆ†æ/analyze_errors.py <database_path>
```

#### Position Accuracy Analysis

Analyze position accuracy using LCS (Longest Common Subsequence) algorithm:

```bash
python æ•°æ®åˆ†æ/analyze_position_accuracy.py <database_path>
```

#### Generate Visualization Heatmaps

```bash
# Generate all heatmaps
python æ•°æ®åˆ†æ/generate_all_heatmaps.py <database_path>

# Or generate individually
python æ•°æ®åˆ†æ/create_heatmap.py <position_accuracy_database_path>
python æ•°æ®åˆ†æ/create_hallucination_heatmap.py <error_stats_database_path>
python æ•°æ®åˆ†æ/create_missing_heatmap.py <error_stats_database_path>
```

### 4. Evaluate Single Test

```bash
python evaluate_test.py
```

This command evaluates the accuracy of [`test.json`](test.json) relative to [`ç­”æ¡ˆ.json`](ç­”æ¡ˆ.json).

### Database Storage

Test results are stored in SQLite databases, separated by byte count:
- **Raw Data**: `bytes_{byte_count}` tables store raw test records
- **Statistical Summary**: `bytes_stats` table records answered count and parse failure count

### Analysis Results

Analysis results are stored in separate databases:
- **Model Overview**: `model_summary_{model_id}.db`
- **Error Statistics**: `error_stats_{model_id}.db`
- **Position Accuracy**: `position_accuracy_{model_id}.db`

### Visualization Charts

The project generates various visualization charts (stored in each model's `test_results/` subdirectory):
- ğŸ“Š **Average Accuracy Score Chart**: Shows average accuracy at different byte counts
- ğŸ”¥ **Position Accuracy Heatmap**: Shows correct answer probability at each position
- ğŸ¯ **Hallucination Test Heatmap**: Shows distribution of hallucination errors
- ğŸ“‰ **Missing Test Heatmap**: Shows distribution of missing errors
- ğŸ“ˆ **JSON Output Failure Probability Chart**: Shows probability distribution of parse failures
- ğŸ“‘ **Statistical Tables**: Detailed statistics in Excel format

## Technical Features

### Error Analysis Methods

The project uses algorithms to analyze three types of errors:

1. **Misorder Errors**: Uses LCS algorithm to identify correctly ordered numbers as anchors; correct values not in anchors are misorders
2. **Hallucination Errors**: Identifies numbers output by the model that don't exist in standard answers, and locates their intervals between anchors
3. **Missing Errors**: Counts numbers present in standard answers but not correctly output by the model

### Position Accuracy Algorithm

Based on the **Longest Common Subsequence (LCS)** algorithm:
- Finds subsequences in model responses that are completely consistent with standard answer order
- Counts the frequency of correct identification at each position
- Generates position accuracy distribution charts

## Environment Requirements

### Python Dependencies

```bash
pip install aiohttp numpy matplotlib seaborn openpyxl
```

### Main Libraries
- `aiohttp`: Async HTTP requests
- `sqlite3`: Database operations (Python built-in)
- `numpy`: Numerical computation
- `matplotlib`: Chart plotting
- `seaborn`: Heatmap visualization
- `openpyxl`: Excel file operations

## Known Limitations

### âš ï¸ Important Note: Scope of Test Results

**These tests only represent results from inserting needles within `a|` repetitive sequences.** In actual usage, model recall and attention are significantly influenced by the input text characteristics. Therefore, the accuracy measured in these tests **has comparative value only, not absolute value**.

**Do not directly apply these test results to claim that a model's recall and attention at a certain context length are limited to what these results show.** Different input content, text structures, and linguistic features can all significantly affect actual model performance.

**These tests are suitable for:**
- **Horizontal comparison**: Comparing different models under identical testing conditions
- **Longitudinal comparison**: Evaluating the same model's recall performance across different context window sizes

### DeepSeek Model Testing Notes

For DeepSeek models, since the **DeepSeek V3.1** version, a special attention mechanism has been introduced internally. This causes the test results to be inaccurate when using the simple `a|` pattern â€” specifically, it shows **abnormally high accuracy across the entire context range, which is not normal behavior**.

When we changed the insertion pattern and stopped using the simple `a|` repetition, DeepSeek models showed drastically different results. This suggests that the model may have special optimizations for specific repetitive patterns.

**Future Plans**: We will create a revised version of the test set to more accurately evaluate models with special attention mechanisms, including DeepSeek.

### GPT-5 Model Testing Notes

Additionally, we tested the **GPT-5** model. However, GPT-5 uses an internal **model routing mechanism** that causes severe instability issues:

- In the same context (50k tokens) tests, accuracy distribution is extremely unstable, **ranging wildly from 5% to 100%**
- 100% accuracy likely indicates routing to the best model in the ensemble, but this cannot be reliably reproduced
- We attempted various methods to stabilize routing results, including:
  - Modifying prompt content
  - Adjusting thinking intensity settings (even when set to `high`)
  - Various other optimization strategies

**Test Conclusion**: Due to the inability to obtain stable and reproducible test results, GPT-5 model test sets will not be publicly released at this time. We will publish the test set once better testing methodologies are developed.

## Citation

If this project helps your research, citations are welcome!

---

**Last Updated**: 2025.11.01
